GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
/Users/fevenz/miniforge3/envs/chess_scanner/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  rank_zero_warn(
  | Name      | Type             | Params
-----------------------------------------------
0 | conv1     | Conv2d           | 80
1 | pool      | MaxPool2d        | 0
2 | conv2     | Conv2d           | 1.2 K
3 | fc1       | Linear           | 13.3 K
4 | criterion | CrossEntropyLoss | 0
5 | train_acc | Accuracy         | 0
6 | val_acc   | Accuracy         | 0
7 | test_acc  | Accuracy         | 0
-----------------------------------------------
14.6 K    Trainable params
0         Non-trainable params
14.6 K    Total params
0.058     Total estimated model params size (MB)

Validation sanity check:   0%|                                                                                                                                   | 0/2 [00:00<?, ?it/s]average val loss 2.5702189207077026
/Users/fevenz/miniforge3/envs/chess_scanner/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Epoch 0:   0%|                                                                                                                                                 | 0/517 [00:00<?, ?it/s]
/Users/fevenz/miniforge3/envs/chess_scanner/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.






Epoch 0:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████████▎      | 485/517 [00:14<00:00, 33.94it/s, loss=0.00539, v_num=xjsj]







Epoch 1: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▌| 515/517 [00:14<00:00, 34.53it/s, loss=0.00957, v_num=xjsj]average val loss 0.012992849683884146







Epoch 2:  98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 507/517 [00:15<00:00, 32.65it/s, loss=0.0104, v_num=xjsj]average val loss 0.012828265372488488








Epoch 3:  93%|████████████████████████████████████████████████████████████████████████████████████████████████████▍       | 481/517 [00:19<00:01, 25.27it/s, loss=0.000254, v_num=xjsj]










Epoch 4:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▉  | 507/517 [00:20<00:00, 24.34it/s, loss=0.00132, v_num=xjsj]average val loss 0.014834803630431877
Epoch 4: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 517/517 [00:21<00:00, 24.58it/s, loss=0.00132, v_num=xjsj]